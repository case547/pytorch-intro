{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torch.nn.Module` and `torch.nn.Parameter`\n",
    "\n",
    "One important behaviours of `torch.nn.Module`: registering parameters, e.g. learning weights for `Module` subclass would be instances of `torch.nn.Parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n",
      "\n",
      "Layer 2:\n",
      "Linear(in_features=200, out_features=10, bias=True)\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0410,  0.0239, -0.0756,  ...,  0.0793,  0.0449,  0.0881],\n",
      "        [-0.0026,  0.0443, -0.0817,  ..., -0.0576,  0.0830,  0.0984],\n",
      "        [-0.0799,  0.0598,  0.0191,  ...,  0.0053, -0.0855,  0.0849],\n",
      "        ...,\n",
      "        [-0.0841, -0.0678,  0.0033,  ...,  0.0114, -0.0412,  0.0588],\n",
      "        [ 0.0691, -0.0117, -0.0402,  ...,  0.0024, -0.0211,  0.0092],\n",
      "        [ 0.0369,  0.0289, -0.0612,  ...,  0.0120,  0.0480, -0.0913]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0513,  0.0253, -0.0273,  0.0580,  0.0653,  0.0655,  0.0396,  0.0080,\n",
      "         0.0801, -0.0178, -0.0042,  0.0803,  0.0829, -0.0999, -0.0732, -0.0672,\n",
      "         0.0544,  0.0312, -0.0453,  0.0924,  0.0397, -0.0063,  0.0990,  0.0061,\n",
      "        -0.0980,  0.0216, -0.0892, -0.0722, -0.0531,  0.0414,  0.0110,  0.0269,\n",
      "         0.0884,  0.0295,  0.0986, -0.0700,  0.0017, -0.0809,  0.0663,  0.0176,\n",
      "        -0.0961,  0.0970, -0.0495, -0.0056,  0.0346, -0.0094, -0.0079, -0.0678,\n",
      "         0.0085,  0.0049,  0.0647, -0.0493, -0.0507,  0.0328, -0.0289,  0.0131,\n",
      "         0.0166,  0.0178, -0.0399, -0.0855,  0.0982, -0.0365,  0.0187, -0.0230,\n",
      "         0.0184, -0.0806,  0.0439,  0.0938,  0.0657,  0.0789, -0.0133, -0.0277,\n",
      "        -0.0904,  0.0618,  0.0903, -0.0694, -0.0387,  0.0329,  0.0940, -0.0906,\n",
      "         0.0882, -0.0435, -0.0970,  0.0122,  0.0364,  0.0032, -0.0694,  0.0786,\n",
      "         0.0923, -0.0337,  0.0190,  0.0902, -0.0475, -0.0438, -0.0507,  0.0238,\n",
      "         0.0401,  0.0310, -0.0960,  0.0714, -0.0065, -0.0156, -0.0283, -0.0176,\n",
      "         0.0497,  0.0474, -0.0086, -0.0703,  0.0823, -0.0287, -0.0358,  0.0851,\n",
      "         0.0476,  0.0601,  0.0147,  0.0967,  0.0584, -0.0401, -0.0620, -0.0952,\n",
      "         0.0953,  0.0335, -0.0748, -0.0526, -0.0109, -0.0528,  0.0599,  0.0883,\n",
      "         0.0691, -0.0386,  0.0664,  0.0355,  0.0262, -0.0784, -0.0026,  0.0290,\n",
      "        -0.0078, -0.0820,  0.0986,  0.0661, -0.0301,  0.0138, -0.0825,  0.0956,\n",
      "         0.0431, -0.0577,  0.0479,  0.0393, -0.0038,  0.0111,  0.0674, -0.0916,\n",
      "        -0.0485, -0.0140, -0.0712, -0.0779, -0.0168, -0.0031,  0.0097, -0.0103,\n",
      "        -0.0556,  0.0388,  0.0201, -0.0269,  0.0301, -0.0243,  0.0315, -0.0660,\n",
      "         0.0670, -0.0038, -0.0362,  0.0746, -0.0189,  0.0573,  0.0932,  0.0515,\n",
      "        -0.0644,  0.0607,  0.0107, -0.0705, -0.0378,  0.0888,  0.0082, -0.0118,\n",
      "         0.0898,  0.0997, -0.0027,  0.0228, -0.0496, -0.0726, -0.0429,  0.0944,\n",
      "         0.0186, -0.0275, -0.0036,  0.0409,  0.0881,  0.0039, -0.0650, -0.0970],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0486, -0.0436, -0.0686,  ..., -0.0662, -0.0072,  0.0237],\n",
      "        [-0.0433, -0.0551, -0.0199,  ..., -0.0471,  0.0384, -0.0027],\n",
      "        [ 0.0077, -0.0566, -0.0380,  ...,  0.0170,  0.0179, -0.0127],\n",
      "        ...,\n",
      "        [-0.0665, -0.0707, -0.0491,  ..., -0.0503, -0.0398, -0.0516],\n",
      "        [ 0.0189, -0.0322,  0.0126,  ..., -0.0422, -0.0640,  0.0284],\n",
      "        [ 0.0369,  0.0473,  0.0026,  ..., -0.0483,  0.0084, -0.0370]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0029, -0.0503, -0.0696, -0.0150,  0.0664,  0.0119, -0.0287,  0.0597,\n",
      "         0.0361, -0.0042], requires_grad=True)\n",
      "\n",
      "Layer 2 params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0486, -0.0436, -0.0686,  ..., -0.0662, -0.0072,  0.0237],\n",
      "        [-0.0433, -0.0551, -0.0199,  ..., -0.0471,  0.0384, -0.0027],\n",
      "        [ 0.0077, -0.0566, -0.0380,  ...,  0.0170,  0.0179, -0.0127],\n",
      "        ...,\n",
      "        [-0.0665, -0.0707, -0.0491,  ..., -0.0503, -0.0398, -0.0516],\n",
      "        [ 0.0189, -0.0322,  0.0126,  ..., -0.0422, -0.0640,  0.0284],\n",
      "        [ 0.0369,  0.0473,  0.0026,  ..., -0.0483,  0.0084, -0.0370]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0029, -0.0503, -0.0696, -0.0150,  0.0664,  0.0119, -0.0287,  0.0597,\n",
      "         0.0361, -0.0042], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print(\"The model:\")\n",
    "print(tinymodel)\n",
    "\n",
    "print(\"\\nLayer 2:\")\n",
    "print(tinymodel.linear2)\n",
    "\n",
    "print(\"\\nModel params:\")\n",
    "for param in tinymodel.parameters():\n",
    "    print(param)\n",
    "\n",
    "print(\"\\nLayer 2 params:\")\n",
    "for param in tinymodel.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Layer Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layers\n",
    "\n",
    "Linear layers fully connected; every input influences every layer output to degree specified by layer's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.1872, 0.3584, 0.8722]])\n",
      "\n",
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[-0.2549, -0.3049,  0.4524],\n",
      "        [ 0.0330,  0.0098, -0.3964]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2718, -0.3738], requires_grad=True)\n",
      "\n",
      "Output:\n",
      "tensor([[-0.0343, -0.7098]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "\n",
    "print(\"\\nWeight and Bias parameters:\")\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print(\"\\nOutput:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet takes 1x32x32 b&w image.\n",
    "\n",
    "`conv1` input\n",
    "* `in_channels`: no. input channels, i.e. 1 (if RGB, would be 3) \n",
    "* `out_channels`: no. features we want layer to learn, i.e. 6\n",
    "* `kernel_size`: kernel dimensions, i.e. 5x5 (if non-sqaure, use tuple)\n",
    "\n",
    "`conv1` output\n",
    "* *Activation map* - spatial representation of presence of features in input tensor - 6x28x28 tensor, from no. features (6) and valid kernel positions (28x28)\n",
    "* Passed through ReLU activation func, then max pooling layer with 2x2 pooling region\n",
    "* Gives lower-res 6x14x14 activation map version\n",
    "\n",
    "`conv2`\n",
    "* Expects 6 input channels (no. features sought by `conv1`), has 16 out channels and 3x3 kernel\n",
    "* Ouputs 16x12x12 activation map, reduced by pooling layer to 16x6x6\n",
    "* Reshaped to 16\\*6\\*6 = 576-ele vector for next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Layers\n",
    "\n",
    "RNNS used for sequential data, e.g. time-series measurements, natural lang sentences, DNA nucleotides. They maintain *hidden state* that acts as 'memory' for what it has seen in sequence so far.\n",
    "\n",
    "Internal structure for RNN - or variants LSTM (long short-term memory), GRU (gated recurrent unit) - moderately complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(torch.nn.Module):\n",
    "    \"\"\"Classifer to tell if a word is a noun, verb, etc.\"\"\"\n",
    "\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Layers and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manipulation Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3926, 0.6974, 0.1467, 0.0350, 0.0382, 0.9606],\n",
      "         [0.1009, 0.8235, 0.7125, 0.7284, 0.3182, 0.5734],\n",
      "         [0.0473, 0.5504, 0.8472, 0.8232, 0.3161, 0.3695],\n",
      "         [0.8336, 0.7650, 0.9640, 0.7852, 0.6229, 0.7383],\n",
      "         [0.9466, 0.0603, 0.3900, 0.2318, 0.4900, 0.8594],\n",
      "         [0.6215, 0.3917, 0.0095, 0.6899, 0.1740, 0.8187]]])\n",
      "tensor([[[0.8472, 0.9606],\n",
      "         [0.9640, 0.8594]]])\n"
     ]
    }
   ],
   "source": [
    "# Max pooling layers\n",
    "# Reduce tensor by combining cells and assigning max value of input cells to output cell\n",
    "\n",
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[14.4795, 14.4802, 12.8516, 20.0436],\n",
      "         [19.9820, 11.0803,  7.1885, 22.6962],\n",
      "         [21.6681, 19.1464, 24.1293,  8.0033],\n",
      "         [14.8729, 14.7074, 19.7055, 24.5430]]])\n",
      "tensor(16.8486)\n",
      "tensor([[[-0.3610, -0.3607, -0.9581,  1.6798],\n",
      "         [ 0.7498, -0.6568, -1.2717,  1.1787],\n",
      "         [ 0.5566,  0.1475,  0.9557, -1.6598],\n",
      "         [-0.8857, -0.9266,  0.3085,  1.5039]]],\n",
      "       grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor(-2.9802e-08, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Normalisation layers\n",
    "# Re-centre and nomralise output of layer before feeding to another\n",
    "# Benefits include allowing use of higher learning rates without exploding/vanishing gradients\n",
    "\n",
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5742, 0.4417, 0.8778, 0.6903],\n",
      "         [0.7872, 0.7314, 0.0307, 0.8266],\n",
      "         [0.3286, 0.5668, 0.8089, 0.1789],\n",
      "         [0.0035, 0.7300, 0.9208, 0.6146]]])\n",
      "tensor([[[0.0000, 0.7362, 1.4630, 1.1506],\n",
      "         [0.0000, 0.0000, 0.0512, 1.3777],\n",
      "         [0.5476, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 1.2167, 1.5347, 0.0000]]])\n",
      "tensor([[[0.9570, 0.7362, 1.4630, 1.1506],\n",
      "         [0.0000, 0.0000, 0.0512, 1.3777],\n",
      "         [0.5476, 0.0000, 1.3481, 0.0000],\n",
      "         [0.0058, 1.2167, 1.5347, 1.0243]]])\n"
     ]
    }
   ],
   "source": [
    "# Dropout layers\n",
    "# Tool for encouraging sparse represetations in model; pushing it to infer with less data\n",
    "# Randomly set parts of input tensor to 0 DURING TRAINING\n",
    "\n",
    "my_tensor = torch.rand(1,4,4)\n",
    "print(my_tensor)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)  # p defaults to 0.5\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "954a381b949094e8c8d475b8b19b9ec32c83c8caa63c46efb382e322b832ca79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
