{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tensors and Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "rand1 = torch.rand(2, 3)\n",
    "print(rand1)\n",
    "\n",
    "rand2 = torch.rand(2, 3)\n",
    "print(rand2)\n",
    "\n",
    "torch.manual_seed(1729)  # seed has been reset\n",
    "rand3 = torch.rand(2, 3)\n",
    "print(rand3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(rand4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1416, 2.7183],\n",
      "        [1.6180, 0.0073]])\n",
      "tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n",
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n",
    "print(some_constants)\n",
    "\n",
    "some_ints = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n",
    "print(some_ints)\n",
    "\n",
    "more_ints = torch.tensor(((2, 4, 6), [3, 6, 9]))\n",
    "print(more_ints)\n",
    "\n",
    "# torch.tensor creates a copy of the data\n",
    "data = [[1,2,3], [4,5,6]]\n",
    "torch.tensor(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[11.2406, 11.2083, 11.6692],\n",
      "        [18.3283,  0.2118, 18.4972]], dtype=torch.float64)\n",
      "tensor([[11, 11, 11],\n",
      "        [18,  0, 18]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maths & Logic with PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2, 2) + 1\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itâ€™s important to note here that all of the tensors in the previous code cell were of identical shape. What happens when we try to perform a binary operation on tensors if dissimilar shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(3, 2)\n",
    "\n",
    "try:\n",
    "    print(a * b)\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Brief: Tensor Broadcasting\n",
    "\n",
    "The exception to the same-shapes rule is *tensor broadcasting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7178, 0.0287, 0.4166, 0.4081],\n",
      "        [0.0356, 0.8657, 0.7561, 0.1453]])\n",
      "tensor([[1.4356, 0.0575, 0.8333, 0.8162],\n",
      "        [0.0712, 1.7315, 1.5122, 0.2905]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example above, the 1-row, 4-column tensor is multiplied by *both rows* of the 2-row, 4-column tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules for broadcasting are:\n",
    "* Each tensor must have at least one dimension - no empty tensors.\n",
    "* Comparing the dimension sizes of the two tensors, *going from last to first:*\n",
    "    * Each dimension must be equal, *or*\n",
    "    * One of the dimensions must be of size 1, *or*\n",
    "    * The dimension does not exist in one of the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2742, 0.2243],\n",
      "         [0.4751, 0.3848],\n",
      "         [0.0053, 0.5475]],\n",
      "\n",
      "        [[0.2742, 0.2243],\n",
      "         [0.4751, 0.3848],\n",
      "         [0.0053, 0.5475]],\n",
      "\n",
      "        [[0.2742, 0.2243],\n",
      "         [0.4751, 0.3848],\n",
      "         [0.0053, 0.5475]],\n",
      "\n",
      "        [[0.2742, 0.2243],\n",
      "         [0.4751, 0.3848],\n",
      "         [0.0053, 0.5475]]])\n",
      "tensor([[[0.2765, 0.2765],\n",
      "         [0.0632, 0.0632],\n",
      "         [0.9048, 0.9048]],\n",
      "\n",
      "        [[0.2765, 0.2765],\n",
      "         [0.0632, 0.0632],\n",
      "         [0.9048, 0.9048]],\n",
      "\n",
      "        [[0.2765, 0.2765],\n",
      "         [0.0632, 0.0632],\n",
      "         [0.9048, 0.9048]],\n",
      "\n",
      "        [[0.2765, 0.2765],\n",
      "         [0.0632, 0.0632],\n",
      "         [0.9048, 0.9048]]])\n",
      "tensor([[[0.1875, 0.3266],\n",
      "         [0.1875, 0.3266],\n",
      "         [0.1875, 0.3266]],\n",
      "\n",
      "        [[0.1875, 0.3266],\n",
      "         [0.1875, 0.3266],\n",
      "         [0.1875, 0.3266]],\n",
      "\n",
      "        [[0.1875, 0.3266],\n",
      "         [0.1875, 0.3266],\n",
      "         [0.1875, 0.3266]],\n",
      "\n",
      "        [[0.1875, 0.3266],\n",
      "         [0.1875, 0.3266],\n",
      "         [0.1875, 0.3266]]])\n"
     ]
    }
   ],
   "source": [
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b = a * torch.rand(   3, 2)  # dim 2 & 3 same as a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(    3, 1)  # dim 3 = 1, dim 2 same as a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(    1, 2)  # dim 3 same as a, dim 2 = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\n",
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\n",
      "The size of tensor a (2) must match the size of tensor b (0) at non-singleton dimension 2\n"
     ]
    }
   ],
   "source": [
    "a =         torch.ones(4, 3, 2)\n",
    "\n",
    "try:\n",
    "    b = a * torch.rand(4, 3)\n",
    "except RuntimeError as err:\n",
    "    print(err)  # dimensions must match last-to-first\n",
    "\n",
    "try:\n",
    "    c = a * torch.rand(   2, 3)\n",
    "except RuntimeError as err:\n",
    "    print(err)  # both 3rd & 2nd dims different\n",
    "\n",
    "try:\n",
    "    d = a * torch.rand((0, ))\n",
    "except RuntimeError as err:\n",
    "    print(err)  # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Maths with Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.0820, 0.7429, 0.8884, 0.4138],\n",
      "        [0.5483, 0.8741, 0.2812, 0.2862]])\n",
      "tensor([[1., -0., 1., 1.],\n",
      "        [-0., -0., 1., -0.]])\n",
      "tensor([[ 0., -1.,  0.,  0.],\n",
      "        [-1., -1.,  0., -1.]])\n",
      "tensor([[ 0.0820, -0.5000,  0.5000,  0.4138],\n",
      "        [-0.5000, -0.5000,  0.2812, -0.2862]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Common functions\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print(\"Common functions:\")\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# Trig functions\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# Bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.4818, 0.2718],\n",
      "        [0.2758, 0.1531]])\n",
      "tensor([[1.4453, 0.8154],\n",
      "        [0.8274, 0.4593]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.8687, -0.4954],\n",
      "        [-0.4954,  0.8687]]),\n",
      "S=tensor([1.9103, 0.0057]),\n",
      "V=tensor([[-0.8718,  0.4899],\n",
      "        [-0.4899, -0.8718]]))\n"
     ]
    }
   ],
   "source": [
    "# Comparisons\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)   # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e))  # returns tensor of type bool\n",
    "\n",
    "# Vector and linear algebra\n",
    "v1 = torch.tensor([1., 0., 0.])          # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])          # y unit vector\n",
    "m1 = torch.rand(2, 2)                    # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]])  # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1))  # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                   # 3 times m1\n",
    "print(torch.svd(m3))        # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering Tensors in Place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are times when we may wish to alter a tensor in place. For this, most of the math functions have a version with an appended underscore ( `_` ) that will alter a tensor in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi/4, math.pi/2, 3*math.pi/4])\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(torch.sin(a))  # creates new tensor in memory\n",
    "print(a)             # a hasn't changed\n",
    "\n",
    "b = torch.tensor([0, math.pi/4, math.pi/2, 3*math.pi/4])\n",
    "print(\"\\nb:\")\n",
    "print(b)\n",
    "print(torch.sin_(b))  # added underscore\n",
    "print(b)              # b changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.0713, 0.0398],\n",
      "        [0.8330, 0.0417]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.0713, 1.0398],\n",
      "        [1.8330, 1.0417]])\n",
      "tensor([[1.0713, 1.0398],\n",
      "        [1.8330, 1.0417]])\n",
      "tensor([[0.0713, 0.0398],\n",
      "        [0.8330, 0.0417]])\n",
      "\n",
      "After multiplying:\n",
      "tensor([[0.0051, 0.0016],\n",
      "        [0.6939, 0.0017]])\n",
      "tensor([[0.0051, 0.0016],\n",
      "        [0.6939, 0.0017]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "\n",
    "print(\"Before:\")\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(\"\\nAfter adding:\")\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "print(\"\\nAfter multiplying:\")\n",
    "print(b.mul_(b))\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.3058, 0.3135],\n",
      "        [0.7309, 0.7532]])\n",
      "tensor([[0.0200, 0.6959],\n",
      "        [0.1177, 0.4369]])\n"
     ]
    }
   ],
   "source": [
    "# Many of the methods and functions seen so far have \n",
    "# `out` argument to specify tensor to receive the output\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)  # c contents changed\n",
    "\n",
    "assert c is d         # test c & d are same obj, not just equal in val\n",
    "assert id(c), old_id  # make sure new c sae obj as old one\n",
    "\n",
    "torch.rand(2, 2, out=c)\n",
    "print(c)  # c changed again\n",
    "assert id(c), old_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "# As with any Python object, assigning tensor to var makes var label of tensor; doesn't copy it\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561  # changing a\n",
    "print(b)       # ...also alters b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# To work on separate copy of data, use clone() method\n",
    "\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a      # different objs in memory...\n",
    "print(torch.eq(a, b))  # ...but still same contents\n",
    "\n",
    "a[0][1] = 561  # changing a\n",
    "print(b)       # ...doesn't change b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2196, 0.5041],\n",
      "        [0.7555, 0.4534]], requires_grad=True)\n",
      "tensor([[0.2196, 0.5041],\n",
      "        [0.7555, 0.4534]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.2196, 0.5041],\n",
      "        [0.7555, 0.4534]])\n",
      "tensor([[0.2196, 0.5041],\n",
      "        [0.7555, 0.4534]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# If source tensor has autograd, but don't want clone to track gradients\n",
    "\n",
    "a = torch.rand(2, 2, requires_grad=True)  # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, CPU only.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    if gpu_count == 1:\n",
    "        print(\"We have a GPU!\")\n",
    "    else:\n",
    "        print(f\"We have {gpu_count} GPUs!\")\n",
    "else:\n",
    "    print(\"Sorry, CPU only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, CPU only.\n"
     ]
    }
   ],
   "source": [
    "# Getting data onto target device may be done at creation time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device=\"cuda\")\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print(\"Sorry, CPU only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "tensor([[0.2537, 0.5821],\n",
      "        [0.5127, 0.6171]])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    my_device = torch.device(\"cpu\")\n",
    "print(\"Device:\", my_device)\n",
    "\n",
    "x = torch.rand(2, 2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Moving existing tensor to another device\n",
    "\n",
    "y = torch.rand(2, 2)\n",
    "print(y.device)\n",
    "\n",
    "y = y.to(my_device)\n",
    "print(y.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justin.mak\\OneDrive - Ocean Infinity Group Limited\\Documents\\pytorch-intro\\2_tensors.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justin.mak/OneDrive%20-%20Ocean%20Infinity%20Group%20Limited/Documents/pytorch-intro/2_tensors.ipynb#ch0000040?line=0'>1</a>\u001b[0m \u001b[39m# All tensors in a computation must be on same device\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justin.mak/OneDrive%20-%20Ocean%20Infinity%20Group%20Limited/Documents/pytorch-intro/2_tensors.ipynb#ch0000040?line=2'>3</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/justin.mak/OneDrive%20-%20Ocean%20Infinity%20Group%20Limited/Documents/pytorch-intro/2_tensors.ipynb#ch0000040?line=3'>4</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrand(\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justin.mak/OneDrive%20-%20Ocean%20Infinity%20Group%20Limited/Documents/pytorch-intro/2_tensors.ipynb#ch0000040?line=5'>6</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/justin.mak/OneDrive%20-%20Ocean%20Infinity%20Group%20Limited/Documents/pytorch-intro/2_tensors.ipynb#ch0000040?line=6'>7</a>\u001b[0m     z \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m y\n",
      "File \u001b[1;32mc:\\Users\\justin.mak\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\cuda\\__init__.py:211\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmultiprocessing, you must use the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mspawn\u001b[39m\u001b[39m'\u001b[39m\u001b[39m start method\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(torch\u001b[39m.\u001b[39m_C, \u001b[39m'\u001b[39m\u001b[39m_cuda_getDeviceCount\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTorch not compiled with CUDA enabled\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m _cudart \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[0;32m    214\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# All tensors in a computation must be on same device\n",
    "\n",
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2, device=\"cuda\")\n",
    "\n",
    "try:\n",
    "    z = x + y\n",
    "except RuntimeError as err:\n",
    "    print(\"RuntimeError:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Number of Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# Making batch of N=1\n",
    "\n",
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)  # add dimension \n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.4206, 0.3805, 0.1938, 0.0572, 0.0231, 0.1395, 0.2469, 0.1049, 0.5591,\n",
      "         0.9609, 0.1817, 0.7692, 0.5956, 0.5949, 0.5735, 0.3678, 0.1782, 0.0720,\n",
      "         0.8156, 0.4454]])\n",
      "torch.Size([20])\n",
      "tensor([0.4206, 0.3805, 0.1938, 0.0572, 0.0231, 0.1395, 0.2469, 0.1049, 0.5591,\n",
      "        0.9609, 0.1817, 0.7692, 0.5956, 0.5949, 0.5735, 0.3678, 0.1782, 0.0720,\n",
      "        0.8156, 0.4454])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)  # no change as can only squeeze() dim of extent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2\n",
      "torch.Size([3, 1])\n",
      "tensor([[[0.4772, 0.4772],\n",
      "         [0.3319, 0.3319],\n",
      "         [0.6933, 0.6933]],\n",
      "\n",
      "        [[0.4772, 0.4772],\n",
      "         [0.3319, 0.3319],\n",
      "         [0.6933, 0.6933]],\n",
      "\n",
      "        [[0.4772, 0.4772],\n",
      "         [0.3319, 0.3319],\n",
      "         [0.6933, 0.6933]],\n",
      "\n",
      "        [[0.4772, 0.4772],\n",
      "         [0.3319, 0.3319],\n",
      "         [0.6933, 0.6933]]])\n"
     ]
    }
   ],
   "source": [
    "# Using unsqueeze() to help broadcasting\n",
    "\n",
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)\n",
    "\n",
    "try:\n",
    "    print(a * b)\n",
    "except RuntimeError as err:\n",
    "    print(err)\n",
    "\n",
    "c = b.unsqueeze(1)  # make 2D by adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "# In-place versions\n",
    "\n",
    "batch_me = torch.rand(3, 226, 226)\n",
    "print(batch_me.shape)\n",
    "\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "# Sometimes may want to change tensor shape more radically, e.g. between last conv and linear layer\n",
    "\n",
    "output3d = torch.rand(6, 20, 20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6*20*20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# Can also call as method on torch module\n",
    "print(torch.reshape(output3d, (6*20*20,)).shape)  # tuple expected when specifying tensor shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np_array = np.ones((2,3))\n",
    "print(np_array)\n",
    "\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3272, 0.0603, 0.8408],\n",
      "        [0.9608, 0.8584, 0.4251]])\n",
      "[[0.32717657 0.06030667 0.8408447 ]\n",
      " [0.9607948  0.8584186  0.4251135 ]]\n"
     ]
    }
   ],
   "source": [
    "torch_rand = torch.rand(2, 3)\n",
    "print(torch_rand)\n",
    "\n",
    "np_rand = torch_rand.numpy()\n",
    "print(np_rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.32717657  0.06030667  0.8408447 ]\n",
      " [ 0.9607948  17.          0.4251135 ]]\n"
     ]
    }
   ],
   "source": [
    "# Converted objs using same underlying memory as sources, so changes to one reflected in other\n",
    "\n",
    "np_array[1, 1] = 23\n",
    "print(torch_tensor)\n",
    "\n",
    "torch_rand[1, 1] = 17\n",
    "print(np_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2720f8f7927cabbb594abb5ac9ed583f197d67d5a15459d4ce8a63569bbfaff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
