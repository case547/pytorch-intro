{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Fundamentals of Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we need Autograd for?\n",
    "\n",
    "Treating inputs as n *i*-dimensional vector $\\vec{x}$, model *M* can be expressed as vector-valued function $\\vec{y} = \\vec{M}(\\vec{x})$. From here, the vector sign is omitted where contextually clear.\n",
    "\n",
    "Since mostly discussing autograd in context of training, output of interest is model's loss function $L(\\vec{y}) = L(\\vec{M}(\\vec{x}))$, a single-valued scalar function of model output expressing how far off model's prediction was from particular input's *ideal* output.\n",
    "\n",
    "Minimising loss involves iteratively nudging learning weights until tolerable loss produced for variety of inputs.\n",
    "\n",
    "This means making $\\frac{\\partial L}{\\partial x} = 0$, which should be re-expressed to be derived from a funciton of the model output $\\frac{\\partial L(\\vec{y})}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{\\partial M(x)}{\\partial x}$.\n",
    "\n",
    "Across many local partial derivatives within $\\frac{\\partial M(x)}{\\partial x}$, gradients over learning weights most of interest - tell us *what direction to change each weight* to get loss function closer to zero.\n",
    "\n",
    "Since number of local derivates (each corresponding to separate path through model's computation graph) goes up exponentially with network depth, so does comlexity in computing them. Autograd tracks history of every computation, greatly speeding up learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "# In every computation that follows, autograd will accum history of computation in output tensors\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b) \n",
    "# grad_fn hints that executing back-prop step, will need to\n",
    "# compute derivative of sin(x) for all this this tensor's inpurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 2*b\n",
    "print(c)\n",
    "\n",
    "d = c+1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-element output, as would be needed to call .backward() on tensor w/o args\n",
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      "<AddBackward0 object at 0x0000023CEDBC53A0>\n",
      "((<MulBackward0 object at 0x0000023CEDCD3640>, 0), (None, 0))\n",
      "((<SinBackward0 object at 0x0000023CEDBC53A0>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x0000023CEDCD3580>, 0),)\n",
      "()\n",
      "\n",
      "c:\n",
      "<MulBackward0 object at 0x0000023CEDC40B20>\n",
      "\n",
      "b:\n",
      "<SinBackward0 object at 0x0000023CEDC40B20>\n",
      "\n",
      "a:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Each grad_fn stored allows walking down computation to inputs with `next_functions` property\n",
    "\n",
    "print(\"d:\")\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "\n",
    "print(\"\\nc:\")\n",
    "print(c.grad_fn)\n",
    "\n",
    "print(\"\\nb:\")\n",
    "print(b.grad_fn)\n",
    "\n",
    "print(\"\\na:\")\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
      "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
      "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
      "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
      "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23cedd08790>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get derivates out\n",
    "\n",
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cmoputation steps taken to get here\n",
    "\n",
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "b = torch.sin(a)\n",
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only compuation's *leaf nodes* have gradients computed. `print(c.grad)` would return `None`. Here only input is leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd in Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        # No need to specify autograd because assumed true in nn.Module subclass\n",
    "        self.layer1 = torch.nn.Linear(1000, 100)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0462,  0.0134,  0.0492, -0.0561,  0.0096,  0.0521,  0.0900,  0.0649,\n",
      "        -0.0899, -0.0258], grad_fn=<SliceBackward0>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Examine weights and verify no gradients computed yet\n",
    "\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(166.5551, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run through one training batch\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "prediction = model(some_input)\n",
    "\n",
    "loss = (ideal_output - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0462,  0.0134,  0.0492, -0.0561,  0.0096,  0.0521,  0.0900,  0.0649,\n",
      "        -0.0899, -0.0258], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.9128, -1.3424,  1.2112,  0.3252, -1.3144,  0.0086, -3.2288,  0.5864,\n",
      "        -1.2520,  0.2413])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0471,  0.0148,  0.0480, -0.0564,  0.0109,  0.0521,  0.0932,  0.0644,\n",
      "        -0.0886, -0.0260], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.9128, -1.3424,  1.2112,  0.3252, -1.3144,  0.0086, -3.2288,  0.5864,\n",
      "        -1.2520,  0.2413])\n"
     ]
    }
   ],
   "source": [
    "# Gradients computed for each weight but weights not changed because haven't run optimiser yet\n",
    "\n",
    "optimiser.step()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9128, -1.3424,  1.2112,  0.3252, -1.3144,  0.0086, -3.2288,  0.5864,\n",
      "        -1.2520,  0.2413])\n",
      "tensor([ -8.2824,  -5.8159,  -1.7837,   5.9662,  -4.6105,   2.1204, -14.6650,\n",
      "          7.2954,   2.2307,  -2.4608])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# After running optimiser.step(), need to call optimiser.zero_grad() to\n",
    "# stop acummuclating grad on learning weights after every loss.backward()\n",
    "\n",
    "# Example of not doing this\n",
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "for i in range(5):\n",
    "    pred = model(some_input)\n",
    "    loss = (ideal_output - pred).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])  # gradient magnitudes much larger\n",
    "\n",
    "optimiser.zero_grad()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Autograd Off and On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# Simplest: require_grad\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Temporary: torch.no_grad()\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# torch.no_grad() used as func/method decorator:\n",
    "\n",
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8464, 0.7784, 0.8706, 0.0292, 0.1628], requires_grad=True)\n",
      "tensor([0.8464, 0.7784, 0.8706, 0.0292, 0.1628])\n"
     ]
    }
   ],
   "source": [
    "# Have tensor that needs grad tracking but want copy that doesn't\n",
    "\n",
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd and In-place Operations\n",
    "\n",
    "In every example in this notebook so far, weâ€™ve used variables to capture the intermediate values of a computation. Autograd needs these intermediate values to perform gradient computations. *For this reason, you must be careful about using in-place operations when using autograd.* Doing so can destroy information you need to compute derivatives in the `backward()` call. PyTorch will even stop you if you attempt an in-place operation on leaf variable that requires autograd, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "\n",
    "try:\n",
    "    torch.sin_(a)\n",
    "except RuntimeError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd Profiler\n",
    "\n",
    "Autograd's computation tracking plus timing information would make a handy profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "         Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "    aten::div        66.46%      10.941ms        66.46%      10.941ms      10.941us          1000  \n",
      "    aten::mul        33.54%       5.522ms        33.54%       5.522ms       5.522us          1000  \n",
      "-------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 16.463ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile() as prf:\n",
    "    for _ in range(1000):\n",
    "        z = (z/x)*y\n",
    "\n",
    "print(prf.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Autograd Detail and the High-Level API\n",
    "\n",
    "Imagine PyTorch model as function with n-dimensional input and m-dimensional output $\\vec{y} = f(\\vec{x})$ and jacobian $J$.\n",
    "\n",
    "Loss function $l = g(\\vec{y})$ that returns a scalar output would have gradients as column vector $v = \\Big(\\frac{\\partial l}{\\partial y_1} \\cdots \\frac{\\partial l}{\\partial y_m}\\Big)^T$.\n",
    "\n",
    "$$v^T \\cdot J = \\Bigg(\\frac{\\partial l}{\\partial y_1} \\cdots \\frac{\\partial l}{\\partial y_m}\\Bigg) \\cdot \\underline{J} = \\Bigg(\\frac{\\partial l}{\\partial x_1} \\cdots \\frac{\\partial l}{\\partial x_n}\\Bigg)$$\n",
    "\n",
    "**`torch.autograd` computes these (Jacobian-vector) products,** and is how gradients over learning weights are accumulated during backward pass.\n",
    "\n",
    "`backward()` call can also take optional vector input representing set of gradients over (output) tensor, which are multiplied by Jacobian of autograd-traced tensor preceding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 571.7303, 1491.2374,   28.9516], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad can be implicitly created only for scalar outputs\n",
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    y.backward()\n",
    "except RuntimeError as err:\n",
    "    # For multi-dim output, need to provide their\n",
    "    # gradients for autograd to multiply into Jacobian\n",
    "    print(err)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The High-Level API\n",
    "\n",
    "There is API on autograd that gives direct access to important differentiqal matrix and vector ops, notably calculating Jacobian and Hessian of particular function and inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inputs: (tensor([0.9878]), tensor([0.0714]))\n",
      "Jacobian: (tensor([[5.3709]]), tensor([[3.]]))\n",
      " Hessian: ((tensor([[5.3709]]), tensor([[0.]])), (tensor([[0.]]), tensor([[0.]])))\n"
     ]
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1))  # function args\n",
    "print(\"  Inputs:\", inputs)\n",
    "\n",
    "print(\"Jacobian:\", torch.autograd.functional.jacobian(exp_adder, inputs))  # 2e^x and 3\n",
    "print(\" Hessian:\", torch.autograd.functional.hessian(exp_adder, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([514.8537, 653.6339, 811.2714]),\n",
       " tensor([1.0240e+02, 1.0240e+03, 1.0240e-01]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly computing vector-Jacobian product, if vector provided\n",
    "\n",
    "def do_some_doubling(x):\n",
    "    y = x*2\n",
    "    while y.data.norm() < 1000:\n",
    "        y *= 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.rand(3)\n",
    "grads = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, grads)  # jvp() same with operands reversed\n",
    "# vhp() and hvp() also exist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "954a381b949094e8c8d475b8b19b9ec32c83c8caa63c46efb382e322b832ca79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
